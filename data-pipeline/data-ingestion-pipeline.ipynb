{
 "cells": [
  {
   "cell_type": "code",
   "id": "ae70d15e34a5ea9a",
   "metadata": {},
   "source": [
    "#Read data\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Extract & Transform\n",
    "import kagglehub\n",
    "import ast\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# Normalize locations\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderQuotaExceeded\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "#Load\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import IntegrityError, SQLAlchemyError\n",
    "import psycopg2\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b03dc890fff648f1",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f92976e28f579d2",
   "metadata": {},
   "source": [
    "load_dotenv()\n",
    "app_token = os.getenv(\"APP_TOKEN\")\n",
    "username = os.getenv(\"OPEN_DATA_NYC_USERNAME\")\n",
    "password = os.getenv(\"OPEN_DATA_NYC_PASSWORD\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def fetch_restaurant_data(app_token, username, password, dataset_id=\"pitm-atqc\", limit=1000):\n",
    "    \"\"\"\n",
    "    Fetch restaurant data from the NYC Open Data API.\n",
    "\n",
    "    Parameters:\n",
    "        app_token (str): Your application token for the API.\n",
    "        username (str): Your username for the API (email).\n",
    "        password (str): Your password for the API.\n",
    "        dataset_id (str): The dataset identifier in Socrata.\n",
    "        limit (int): The maximum number of results to fetch (default is 1000).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the restaurant data.\n",
    "    \"\"\"\n",
    "    # Initialize the Socrata client\n",
    "    client = Socrata(\"data.cityofnewyork.us\", app_token, username=username, password=password)\n",
    "\n",
    "    # Fetch data\n",
    "    results = client.get(dataset_id, limit=limit)\n",
    "\n",
    "    # Convert results to a pandas DataFrame\n",
    "    df_restaurants = pd.DataFrame.from_records(results)\n",
    "    print(f\"Fetched {len(df_restaurants)} rows from the API.\")\n",
    "\n",
    "    return df_restaurants"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "abac52feade3d8d0",
   "metadata": {},
   "source": [
    "def parse_selected_sheets(file_path, sheet_names, header_row_mapping):\n",
    "    \"\"\"\n",
    "    Parse selected worksheets from an XML-based Excel workbook.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the XML file.\n",
    "        sheet_names (list): List of worksheet names to parse.\n",
    "        header_row_mapping (dict): A mapping of sheet names to header row indices.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of pandas DataFrames corresponding to the selected sheets.\n",
    "    \"\"\"\n",
    "    def extract_row_data(cells, expected_columns):\n",
    "        \"\"\"\n",
    "        Helper function to extract row data and ensure it matches the number of headers.\n",
    "        \"\"\"\n",
    "        row_data = []\n",
    "        for i in range(expected_columns):\n",
    "            try:\n",
    "                cell = cells[i].find(\".//ss:Data\", ns)\n",
    "                row_data.append(cell.text.strip() if cell is not None else None)\n",
    "            except IndexError:\n",
    "                row_data.append(None)  # Append None if the column is missing\n",
    "        return row_data\n",
    "\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Namespace dictionary for handling XML namespaces\n",
    "    ns = {'ss': 'urn:schemas-microsoft-com:office:spreadsheet'}\n",
    "\n",
    "    # Retrieve all worksheets\n",
    "    worksheets = root.findall(\".//ss:Worksheet\", ns)\n",
    "\n",
    "    # List to store DataFrames for selected sheets\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through each worksheet\n",
    "    for sheet in worksheets:\n",
    "        sheet_name = sheet.attrib.get(f\"{{{ns['ss']}}}Name\")  # Get the sheet name\n",
    "        if sheet_name not in sheet_names:\n",
    "            continue  # Skip sheets that are not in the specified list\n",
    "\n",
    "        rows = sheet.findall(\".//ss:Row\", ns)  # Find all rows in the sheet\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"Sheet '{sheet_name}' is empty. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Determine the header row index (default to 0 if not specified)\n",
    "        header_row_index = header_row_mapping.get(sheet_name, 0)\n",
    "        if header_row_index >= len(rows):\n",
    "            print(f\"Invalid header row index for sheet: {sheet_name}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Extract headers from the specified row\n",
    "        header_row = rows[header_row_index]\n",
    "        headers = []\n",
    "        for cell in header_row.findall(\".//ss:Cell\", ns):\n",
    "            data = cell.find(\".//ss:Data\", ns)\n",
    "            headers.append(data.text.strip() if data is not None else None)\n",
    "        expected_columns = len(headers)\n",
    "\n",
    "        # Extract data (skip up to the header row)\n",
    "        data = []\n",
    "        for row in rows[header_row_index + 1:]:  # Start after the header row\n",
    "            cells = row.findall(\".//ss:Cell\", ns)\n",
    "            row_data = extract_row_data(cells, expected_columns)\n",
    "            data.append(row_data)\n",
    "\n",
    "        # Create a DataFrame for the sheet\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        dataframes.append(df)  # Add the DataFrame to the list\n",
    "\n",
    "        print(f\"Processed sheet: {sheet_name} with {len(df)} rows\")\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "def fetch_fliming_locations_data(xml_file_path):\n",
    "    \"\"\"\n",
    "    Fetch the \"Full Map List\" worksheet as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        xml_file_path (str): Path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing data from the \"Full Map List\" worksheet.\n",
    "    \"\"\"\n",
    "    # Sheet names of interest\n",
    "    selected_sheets = ['Full Map List']\n",
    "\n",
    "    # Header row mapping for each sheet\n",
    "    header_row_mapping = {\n",
    "        'Full Map List': 1\n",
    "    }\n",
    "\n",
    "    # Parse the selected sheets\n",
    "    dfs = parse_selected_sheets(xml_file_path, selected_sheets, header_row_mapping)\n",
    "    df_filming_locations = dfs[0]  # Extract the DataFrame for the 'Full Map List' sheet\n",
    "\n",
    "    return df_filming_locations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3ba7c57285e02db",
   "metadata": {},
   "source": [
    "def fetch_movies_data(kaggle_dataset, filename=\"25k IMDb movie Dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Download the latest version of a Kaggle dataset and return the movies DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        kaggle_dataset (str): The Kaggle dataset identifier (e.g., \"utsh0dey/25k-movie-dataset\").\n",
    "        filename (str): The name of the CSV file to load (default is \"25k IMDb movie Dataset.csv\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the movies data.\n",
    "    \"\"\"\n",
    "    # Download the latest version of the dataset\n",
    "    path = kagglehub.dataset_download(kaggle_dataset)\n",
    "    csv_path = f\"{path}/{filename}\"\n",
    "    df_movies = pd.read_csv(csv_path)\n",
    "\n",
    "    return df_movies"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b94ccaff33c0604",
   "metadata": {},
   "source": [
    "df_restaurants_raw = fetch_restaurant_data(app_token, username, password)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec165c068565e82b",
   "metadata": {},
   "source": [
    "xml_file_path = \"./datasets/Interactive_Map_Data.xml\"\n",
    "df_fl_raw = fetch_fliming_locations_data(xml_file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "beee16c73f9b021b",
   "metadata": {},
   "source": [
    "kaggle_dataset = \"utsh0dey/25k-movie-dataset\"\n",
    "df_movies_raw = fetch_movies_data(kaggle_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "89d58a8d8f268f0f",
   "metadata": {},
   "source": [
    "# Data preparation and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92936d43f3ccac81",
   "metadata": {},
   "source": [
    "## Movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "dfbdad920fc274e",
   "metadata": {},
   "source": [
    "def extract_movie_id(df, path_column='path', new_column='imdb_id'):\n",
    "    \"\"\"\n",
    "    Extract the unique movie ID from the 'path' field and add it as a new column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the 'path' column.\n",
    "        path_column (str): The name of the column containing the path (default: 'path').\n",
    "        new_column (str): The name of the new column for the extracted movie ID (default: 'movie_id').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with the extracted movie ID column.\n",
    "    \"\"\"\n",
    "    # Check if the path column exists\n",
    "    if path_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{path_column}' not found in the DataFrame.\")\n",
    "\n",
    "    # Use regex to extract the movie ID from the path\n",
    "    df[new_column] = df[path_column].str.extract(r'/title/(tt\\d+)/')\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4bac892ef18bae06",
   "metadata": {},
   "source": [
    "def extract_year(df, column_name='year'):\n",
    "    \"\"\"\n",
    "    Extract four-digit year from a given column in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the year column.\n",
    "        column_name (str): The name of the column to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with the year column cleaned.\n",
    "    \"\"\"\n",
    "    # Extract the four-digit year using regex\n",
    "    df[column_name] = df[column_name].str.extract(r'(\\b\\d{4}\\b)', expand=False)\n",
    "\n",
    "    # Replace any NaN values with \"null\"\n",
    "    #df[column_name] = df[column_name].astype('object').fillna(None)\n",
    "    #df[column_name] = df[column_name].astype('Int64')  # Pandas nullable integer type\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b07b1769834e4ff",
   "metadata": {},
   "source": [
    "def prepare_column(df, old_column_name, new_column_name):\n",
    "    \"\"\"\n",
    "    Rename a column and convert its values from strings to Python lists.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the column.\n",
    "        old_column_name (str): The current name of the column.\n",
    "        new_column_name (str): The new name for the column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with the renamed and properly formatted column.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the old_column_name is not found in the DataFrame.\n",
    "    \"\"\"\n",
    "    if old_column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{old_column_name}' not found in the DataFrame.\")\n",
    "\n",
    "    df = df.rename(columns={old_column_name: new_column_name})\n",
    "\n",
    "    # Convert column values from string to list\n",
    "    df[new_column_name] = df[new_column_name].apply(ast.literal_eval)  # Safely convert string to list\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_lookup_table(df, column_name, id_column_name, value_column_name):\n",
    "    \"\"\"\n",
    "    Create a lookup table with unique values and their IDs from a column in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the column.\n",
    "        column_name (str): The name of the column to extract unique values from.\n",
    "        id_column_name (str): The name of the ID column in the lookup table.\n",
    "        value_column_name (str): The name of the value column in the lookup table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A lookup table with unique values and their IDs.\n",
    "    \"\"\"\n",
    "    # Extract unique values and assign IDs\n",
    "    unique_values = set(value for values_list in df[column_name] for value in values_list)\n",
    "    lookup_table = pd.DataFrame({value_column_name: sorted(unique_values)})\n",
    "    lookup_table[id_column_name] = lookup_table.index + 1  # Assign unique IDs starting from 1\n",
    "\n",
    "    return lookup_table\n",
    "\n",
    "\n",
    "def create_link_table(df, lookup_table, column_name, id_column_name, movie_id_column, value_column_name):\n",
    "    \"\"\"\n",
    "    Create a link table connecting movies to values (e.g., genres, actors) by their IDs.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The movies DataFrame.\n",
    "        lookup_table (pd.DataFrame): The lookup table with unique values and their IDs.\n",
    "        column_name (str): The name of the column in the movies DataFrame to link.\n",
    "        id_column_name (str): The name of the ID column in the link table.\n",
    "        movie_id_column (str): The name of the unique movie identifier column in the movies DataFrame.\n",
    "        value_column_name (str): The name of the value column in the lookup table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A link table connecting movies (movie_id) to values (e.g., genres, actors) by their IDs.\n",
    "    \"\"\"\n",
    "    # Explode the column into separate rows\n",
    "    df_expanded = df.explode(column_name)\n",
    "\n",
    "    # Map values to their IDs using the lookup table\n",
    "    link_table = (df_expanded[[movie_id_column, column_name]]\n",
    "                  .merge(lookup_table, left_on=column_name, right_on=value_column_name)\n",
    "                  .rename(columns={id_column_name: id_column_name})\n",
    "                  )\n",
    "\n",
    "    # Drop unnecessary columns and return the link table\n",
    "    return link_table[[movie_id_column, id_column_name]]\n",
    "\n",
    "def clean_and_reorder_movies(df):\n",
    "    \"\"\"\n",
    "    Clean and reorder the movies DataFrame by renaming, dropping, and reordering columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input movies DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and reordered movies DataFrame.\n",
    "    \"\"\"\n",
    "    # Rename columns, drop unuseful columns, make lowercase, and reorder\n",
    "    df = (\n",
    "        df.rename(columns={\n",
    "            'movie title': 'title',\n",
    "            'User Rating': 'nb_users_ratings',\n",
    "            'Rating': 'rating'\n",
    "        })\n",
    "        .drop(columns=['Run Time', 'genres', 'Plot Kyeword', 'actors', 'path'])\n",
    "        .pipe(lambda x: x.set_axis(x.columns.str.lower(), axis=1))\n",
    "        .loc[:, ['imdb_id', 'title', 'year', 'director', 'writer', 'overview', 'rating', 'nb_users_ratings']]\n",
    "    )\n",
    "\n",
    "    # Replace \"no-rating\" with None (equivalent to NULL in databases)\n",
    "    df['rating'] = df['rating'].replace('no-rating', None)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_movie_data(df_movies):\n",
    "    \"\"\"\n",
    "    Process the movie dataset through the entire data pipeline:\n",
    "    1. Prepare and clean the genres column.\n",
    "    2. Extract the 4-digit year.\n",
    "    3. Create the Genres lookup table.\n",
    "    4. Create the Movies_Genres link table.\n",
    "    5. Prepare and clean the actors column.\n",
    "    6. Create the Actors lookup table.\n",
    "    7. Create the Movies_Actors link table.\n",
    "    8. Clean and reorder the movies DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df_movies (pd.DataFrame): The raw movies DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - df_movies (pd.DataFrame): Cleaned and reordered movies DataFrame.\n",
    "            - df_genres (pd.DataFrame): Genres lookup table.\n",
    "            - df_movies_genres (pd.DataFrame): Movies_Genres link table.\n",
    "            - df_actors (pd.DataFrame): Actors lookup table.\n",
    "            - df_movies_actors (pd.DataFrame): Movies_Actors link table.\n",
    "    \"\"\"\n",
    "    df_movies = extract_movie_id(df_movies, path_column='path', new_column='imdb_id')\n",
    "    df_movies = prepare_column(df_movies, old_column_name='Generes', new_column_name='genres')\n",
    "    df_movies = extract_year(df_movies, column_name='year')\n",
    "\n",
    "    df_genres = create_lookup_table(df_movies, column_name='genres', id_column_name='genre_id', value_column_name='genre')\n",
    "    df_genres = df_genres.drop_duplicates(subset=['genre'])\n",
    "\n",
    "    df_movies_genres = create_link_table(\n",
    "        df_movies,\n",
    "        df_genres,\n",
    "        column_name='genres',\n",
    "        id_column_name='genre_id',\n",
    "        movie_id_column='imdb_id',\n",
    "        value_column_name='genre'\n",
    "    )\n",
    "    df_movies_genres = df_movies_genres.drop_duplicates(subset=['genre_id', 'imdb_id'])\n",
    "    \n",
    "    df_movies = prepare_column(df_movies, old_column_name='Top 5 Casts', new_column_name='actors')\n",
    "    df_movies = df_movies.drop_duplicates(subset=['imdb_id'])\n",
    "\n",
    "    df_actors = create_lookup_table(\n",
    "        df_movies, column_name='actors', id_column_name='actor_id', value_column_name='actor_name'\n",
    "    )\n",
    "    df_actors = df_actors.drop_duplicates(subset=['actor_name'])\n",
    "\n",
    "    df_movies_actors = create_link_table(\n",
    "        df_movies,\n",
    "        df_actors,\n",
    "        column_name='actors',\n",
    "        id_column_name='actor_id',\n",
    "        movie_id_column='imdb_id',\n",
    "        value_column_name='actor_name'\n",
    "    )\n",
    "    df_movies_actors = df_movies_actors.drop_duplicates(subset=['actor_id', 'imdb_id'])\n",
    "\n",
    "    df_movies = clean_and_reorder_movies(df_movies)\n",
    "\n",
    "    return df_movies, df_genres, df_movies_genres, df_actors, df_movies_actors\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dc51c65a0f1714ed",
   "metadata": {},
   "source": [
    "df_movies = df_movies_raw\n",
    "df_movies_cleaned, df_genres, df_movies_genres, df_actors, df_movies_actors = process_movie_data(df_movies)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce190bb193dd40fc",
   "metadata": {},
   "source": [
    "## Filming Locations"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ab6f28471b4f2ea",
   "metadata": {},
   "source": [
    "def clean_location_text(location_text):\n",
    "    \"\"\"\n",
    "    Clean the Location Display Text field by removing HTML tags, extra spaces, and newlines.\n",
    "\n",
    "    Parameters:\n",
    "        location_text (str): The raw location text.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned location text.\n",
    "    \"\"\"\n",
    "    if not isinstance(location_text, str):\n",
    "        return location_text  # Return as is if not a string\n",
    "\n",
    "    # Remove HTML tags (e.g., <br>)\n",
    "    cleaned_text = re.sub(r'<[^>]*>', ' ', location_text)  # Match any HTML-like tag\n",
    "\n",
    "    # Replace newlines and excessive whitespace with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    # Strip leading and trailing whitespace\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def process_filming_locations(df):\n",
    "    \"\"\"\n",
    "   Process the filming locations dataset, including cleaning the location text.\n",
    "\n",
    "   Parameters:\n",
    "       df (pd.DataFrame): The raw filming locations DataFrame.\n",
    "\n",
    "   Returns:\n",
    "       tuple: A tuple containing two DataFrames:\n",
    "           - df_filming_locations: Contains metadata and IMDB details.\n",
    "           - df_filming_locations_movies: Contains location details.\n",
    "   \"\"\"\n",
    "    # Add a unique identifier for each row\n",
    "    df['location_id'] = range(1, len(df) + 1) #[str(uuid.uuid4()) for _ in range(len(df))]\n",
    "    \n",
    "    # Step 2: Rename 'IMDB LINK' to 'imdb_id' and extract the ID from the URL\n",
    "    df = df.rename(columns={'IMDB LINK': 'imdb_id'})\n",
    "    df['imdb_id'] = df['imdb_id'].apply(\n",
    "        lambda x: re.search(r'tt\\d+', x).group() if isinstance(x, str) and re.search(r'tt\\d+', x) else None\n",
    "    )\n",
    "    \n",
    "    # Step 3: Extract the Director/Filmmaker ID from the 'Director/Filmmaker IMDB Link' column\n",
    "    df['director_imdb_id'] = df['Director/Filmmaker IMDB Link'].apply(\n",
    "        lambda x: re.search(r'nm\\d+', x).group() if isinstance(x, str) and re.search(r'nm\\d+', x) else None\n",
    "    )\n",
    "    \n",
    "    # Step 4: Clean the Location Display Text field\n",
    "    df['Location Display Text'] = df['Location Display Text'].apply(clean_location_text)\n",
    "    \n",
    "    # Step 4: Clean the Location Display Text field\n",
    "    df['Client or book location indicator'] = df['Client or book location indicator'].apply(clean_location_text)\n",
    "\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.rename(columns={\n",
    "        'movie title': 'title',\n",
    "        'location display text': 'address',\n",
    "        'client or book location indicator': 'address_indicator'\n",
    "    })\n",
    "    \n",
    "    # Step 5: Create df_filming_locations\n",
    "    #df_filming_locations = df[['filming_locations_id', 'Director/Filmmaker Name', 'director_imdb_id']].copy()\n",
    "    df_locations = df[['location_id', 'address', 'address_indicator', 'latitude',\n",
    "                               'longitude', 'borough', 'neighborhood']].copy()\n",
    "    \n",
    "    # Step 6: Create df_filming_locations_movies\n",
    "    df_locations_movies = df[['location_id', 'imdb_id']].copy()\n",
    "    \n",
    "    return df_locations, df_locations_movies"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "119aa9102f359c51",
   "metadata": {},
   "source": [
    "df_locations_m, df_locations_movies = process_filming_locations(df_fl_raw)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "23baa46242985d0f",
   "metadata": {},
   "source": [
    "## Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "id": "613fe649100231a6",
   "metadata": {},
   "source": [
    "def process_restaurant_data(df):\n",
    "    \"\"\"\n",
    "    Process the restaurant dataset by cleaning, splitting into relevant DataFrames,\n",
    "    adding unique identifiers, and transforming the landmarkdistrict_terms column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The original restaurant DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (df_restaurants, df_locations, df_restaurants_locations)\n",
    "    \"\"\"\n",
    "    # Step 1: Drop unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        'sidewalk_dimensions_length', 'sidewalk_dimensions_width', 'sidewalk_dimensions_area',\n",
    "        'approved_for_sidewalk_seating', 'approved_for_roadway_seating', 'qualify_alcohol',\n",
    "        'sla_serial_number', 'sla_license_type', 'landmark_district_or_building',\n",
    "        'healthcompliance_terms', 'time_of_submission', 'community_board', 'council_district',\n",
    "        'census_tract', 'bin', 'bbl', 'roadway_dimensions_length', 'roadway_dimensions_width',\n",
    "        'roadway_dimensions_area', 'globalid', 'objectid', 'food_service_establishment',\n",
    "    ]\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')  # Safeguard against missing columns\n",
    "\n",
    "    # Step 2: Add unique identifiers\n",
    "    df['restaurant_id'] = range(1, len(df) + 1)\n",
    "    df['location_id'] = range(234, 234 + len(df)) #[str(uuid.uuid4()) for _ in range(len(df))]\n",
    "\n",
    "    # Step 3: Transform the landmarkdistrict_terms column to boolean\n",
    "    df['landmarkdistrict_terms'] = df['landmarkdistrict_terms'].fillna('false')  # Replace NaN with 'false'\n",
    "    df['landmarkdistrict_terms'] = df['landmarkdistrict_terms'].str.lower().map({'yes': True, 'false': False})\n",
    "\n",
    "    # Fix typo\n",
    "    df = df.rename(columns={\n",
    "        'bulding_number': 'building_number', \n",
    "    })\n",
    "\n",
    "    # Step 4: Create df_restaurants\n",
    "    df_restaurants = df[['restaurant_id', 'location_id', 'restaurant_name', 'legal_business_name', 'doing_business_as_dba', 'seating_interest_sidewalk', 'landmarkdistrict_terms']].copy()\n",
    "\n",
    "    # Step 5: Create df_locations\n",
    "    df_locations = df[['location_id', 'building_number', 'street', 'borough', 'zip',\n",
    "                       'business_address', 'latitude', 'longitude', 'nta']].copy()\n",
    "\n",
    "    return df_restaurants, df_locations\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23e7a76c77123834",
   "metadata": {},
   "source": [
    "df_restaurants, df_locations_r = process_restaurant_data(df_restaurants_raw)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2441359b36f9f940",
   "metadata": {},
   "source": [
    "## Locations (filming and restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "id": "64b7e7f47a45bbe5",
   "metadata": {},
   "source": [
    "def merge_locations(df_filming_locations, df_restaurant_locations):\n",
    "    \"\"\"\n",
    "    Merge two location dataframes (filming and restaurants) into a unified locations dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df_filming_locations (pd.DataFrame): DataFrame containing filming location data.\n",
    "        df_restaurant_locations (pd.DataFrame): DataFrame containing restaurant location data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A unified locations DataFrame.\n",
    "    \"\"\"\n",
    "    # Standardize column names\n",
    "    df_filming_locations = df_filming_locations.rename(columns={\n",
    "        'neighborhood': 'neighborhood_or_nta'\n",
    "    })\n",
    "\n",
    "    df_restaurant_locations = df_restaurant_locations.rename(columns={\n",
    "        'nta': 'neighborhood_or_nta',\n",
    "        'business_address': 'address'\n",
    "    })\n",
    "\n",
    "    # Add source type column\n",
    "    df_filming_locations['source_type'] = 'filming'\n",
    "    df_restaurant_locations['source_type'] = 'restaurant'\n",
    "\n",
    "    # Select relevant columns\n",
    "    df_filming_locations = df_filming_locations[[\n",
    "        'location_id', 'address', 'address_indicator', 'borough', 'neighborhood_or_nta', 'latitude', 'longitude', 'source_type'\n",
    "    ]]\n",
    "    df_restaurant_locations = df_restaurant_locations[[\n",
    "        'location_id', 'building_number', 'street', 'zip', 'borough', 'address', 'neighborhood_or_nta', 'latitude', 'longitude', 'source_type'\n",
    "    ]]\n",
    "\n",
    "    # Concatenate the two dataframes\n",
    "    df_locations = pd.concat([df_filming_locations, df_restaurant_locations], ignore_index=True).loc[:, ['location_id', 'building_number', 'street', 'zip', 'borough', 'address', 'address_indicator', 'neighborhood_or_nta', 'latitude', 'longitude', 'source_type']]\n",
    "\n",
    "    # Deduplicate locations based on lat/lon\n",
    "    #df_locations = df_locations.drop_duplicates(subset=['lat', 'lon']).reset_index(drop=True)\n",
    "\n",
    "    return df_locations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e279cb433dd73263",
   "metadata": {},
   "source": [
    "df_unified_locations = merge_locations(df_locations_m, df_locations_r)\n",
    "\n",
    "#  TODO : Figure out how to handle duplicates locations\n",
    "#df_unified_locations = df_unified_locations.drop_duplicates(subset=['latitude', 'longitude']).reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d77dd80d3efa2b9",
   "metadata": {},
   "source": [
    "### Normalize locations with Geopy"
   ]
  },
  {
   "cell_type": "code",
   "id": "6642237f517f1440",
   "metadata": {},
   "source": [
    "def geocode_location(geolocator, row):\n",
    "    \"\"\"Determine whether to perform reverse or forward geocoding.\"\"\"\n",
    "    if pd.notna(row['latitude']) and pd.notna(row['longitude']):\n",
    "        # Reverse geocoding\n",
    "        result = reverse_geocode_geopy(geolocator, row['latitude'], row['longitude'])\n",
    "        if result:\n",
    "            return process_geocode_result(row, result, status=\"success\")\n",
    "    elif pd.isna(row['latitude']) or pd.isna(row['longitude']):\n",
    "        # Forward geocoding (try multiple address formats)\n",
    "        address_formats = [\n",
    "            f\"{row['street']} {row['zip']} {row['borough']}\",\n",
    "            f\"{row['building_number']} {row['street']} {row['zip']} {row['borough']}\"\n",
    "        ]\n",
    "        for address in address_formats:\n",
    "            result = forward_geocode_geopy(geolocator, address)\n",
    "            if result:\n",
    "                return process_geocode_result(row, result, status=\"success\")\n",
    "    # If all geocoding attempts fail\n",
    "    return process_geocode_result(row, None, status=\"failed\")\n",
    "\n",
    "def reverse_geocode_geopy(geolocator, lat, lon, retries=1, delay=1):\n",
    "    \"\"\"Perform reverse geocoding using Geopy with error handling.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            location = geolocator.reverse((lat, lon), addressdetails=True, timeout=10)\n",
    "            if location:\n",
    "                return location.raw\n",
    "        except (GeocoderTimedOut, GeocoderQuotaExceeded) as e:\n",
    "            print(f\"Error: {e}. Retrying reverse geocode... ({attempt + 1}/{retries})\")\n",
    "        time.sleep(delay * (attempt + 1))\n",
    "    return None\n",
    "\n",
    "def forward_geocode_geopy(geolocator, address, retries=1, delay=1):\n",
    "    \"\"\"Perform forward geocoding using Geopy with error handling.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            location = geolocator.geocode(address, addressdetails=True, timeout=10)\n",
    "            if location:\n",
    "                return location.raw\n",
    "        except (GeocoderTimedOut, GeocoderQuotaExceeded) as e:\n",
    "            print(f\"Error: {e}. Retrying forward geocode... ({attempt + 1}/{retries})\")\n",
    "        time.sleep(delay * (attempt + 1))\n",
    "    return None\n",
    "\n",
    "def process_geocode_result(row, result, status):\n",
    "    \"\"\"Process the geocoding result and return a standardized dictionary.\"\"\"\n",
    "    if status == \"success\" and result:\n",
    "        address_details = result.get('address', {})\n",
    "        return {\n",
    "            'location_id': row['location_id'],\n",
    "            'source_type': row['source_type'],\n",
    "            'status': status,\n",
    "            'failure_reason': None,\n",
    "            'address_type': result.get('type'),\n",
    "            'name': result.get('name'),\n",
    "            'display_name': result.get('display_name'),\n",
    "            'latitude': result.get('lat'),\n",
    "            'longitude': result.get('lon'),\n",
    "            'house_number': address_details.get('house_number'),\n",
    "            'road': address_details.get('road'),\n",
    "            'neighbourhood': address_details.get('neighbourhood'),\n",
    "            'suburb': address_details.get('suburb'),\n",
    "            'county': address_details.get('county'),\n",
    "            'city': address_details.get('city'),\n",
    "            'state': address_details.get('state'),\n",
    "            'ISO3166-2-lvl4': address_details.get('ISO3166-2-lvl4'),\n",
    "            'postcode': address_details.get('postcode'),\n",
    "            'country': address_details.get('country'),\n",
    "            'country_code': address_details.get('country_code')\n",
    "        }\n",
    "    else:\n",
    "        # Handle failed geocoding\n",
    "        return {\n",
    "            'location_id': row['location_id'],\n",
    "            'source_type': row['source_type'],\n",
    "            'status': status,\n",
    "            'failure_reason': 'No response or invalid data' if status == \"failed\" else None,\n",
    "            'address_type': None,\n",
    "            'name': None,\n",
    "            'display_name': None,\n",
    "            'latitude': row['latitude'],\n",
    "            'longitude': row['longitude'],\n",
    "            'house_number': row.get('building_number'),\n",
    "            'road': row.get('street'),\n",
    "            'neighbourhood': row.get('neighborhood_or_nta'),\n",
    "            'suburb': None,\n",
    "            'county': None,\n",
    "            'city': None,\n",
    "            'state': None,\n",
    "            'ISO3166-2-lvl4': None,\n",
    "            'postcode': row.get('zip'),\n",
    "            'country': None,\n",
    "            'country_code': None\n",
    "        }\n",
    "\n",
    "def process_locations_with_geopy(df):\n",
    "    \"\"\"Process all locations in the DataFrame using Geopy.\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"food-and-the-city\")\n",
    "    results = [geocode_location(geolocator, row) for _, row in df.iterrows()]\n",
    "    return pd.DataFrame(results)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "905933b2cec50218",
   "metadata": {},
   "source": [
    "output_file_path = './outputs/standardized_locations.csv'\n",
    "\n",
    "if os.path.exists(output_file_path):\n",
    "    print(f\"File '{output_file_path}' exists. Reading data from the file...\")\n",
    "    df_standardized_locations = pd.read_csv(output_file_path)\n",
    "else:\n",
    "    print(f\"File '{output_file_path}' does not exist. Processing data...\")\n",
    "    df_standardized_locations = process_locations_with_geopy(df_unified_locations)\n",
    "    df_standardized_locations.to_csv(output_file_path, index=False)\n",
    "    print(f\"Processed data saved to '{output_file_path}'.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae228f3aeb4c4450",
   "metadata": {},
   "source": [
    "code to control if all the location have been well transfered from one dataframe to another after transformation\n",
    "```python\n",
    "def check_missing_ids(source_df, target_df, column_name):\n",
    "    \"\"\"\n",
    "    Check if all values in the specified column of the source DataFrame are present in the target DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        source_df (pd.DataFrame): The DataFrame containing the source column to check.\n",
    "        target_df (pd.DataFrame): The DataFrame where the values should be found.\n",
    "        column_name (str): The name of the column to check.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the result of the check.\n",
    "    \"\"\"\n",
    "    # Find missing IDs\n",
    "    missing_ids = set(source_df[column_name]) - set(target_df[column_name])\n",
    "\n",
    "    # Print results\n",
    "    if missing_ids:\n",
    "        print(f\"Count of missing {column_name}: {len(missing_ids)}\")\n",
    "        print(f\"These {column_name} values are missing: {missing_ids}\")\n",
    "    else:\n",
    "        print(f\"All {column_name} values from the source DataFrame are present in the target DataFrame.\")\n",
    "\n",
    "# Check for missing location IDs\n",
    "check_missing_ids(df_locations_m, df_unified_locations, 'location_id')\n",
    "check_missing_ids(df_locations_r, df_unified_locations, 'location_id')\n",
    "check_missing_ids(df_restaurants, df_locations_r, 'location_id')\n",
    "check_missing_ids(df_locations_m, df_unified_locations, 'location_id')\n",
    "\n",
    "# Check for missing location IDs\n",
    "check_missing_ids(df_locations_m, df_standardized_locations, 'location_id')\n",
    "check_missing_ids(df_locations_r, df_standardized_locations, 'location_id')\n",
    "check_missing_ids(df_locations_m, df_standardized_locations, 'location_id')\n",
    "check_missing_ids(df_unified_locations, df_standardized_locations, 'location_id')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e7f52b2127d9bc",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f84f36b3a24a5b2",
   "metadata": {},
   "source": [
    "## Data Fields name Mapping"
   ]
  },
  {
   "cell_type": "code",
   "id": "538af796a9a1fc17",
   "metadata": {},
   "source": [
    "# Mapping for each table\n",
    "mapping_fc_locations = {\n",
    "    'location_id': 'loc_id',\n",
    "    'source_type': 'loc_source_type',\n",
    "    'status': 'loc_status',\n",
    "    'failure_reason': 'loc_failure_reason',\n",
    "    'address_type': 'loc_address_type',\n",
    "    'name': 'loc_name',\n",
    "    'display_name': 'loc_display_name',\n",
    "    'latitude': 'loc_latitude',\n",
    "    'longitude': 'loc_longitude',\n",
    "    'house_number': 'loc_house_number',\n",
    "    'road': 'loc_road',\n",
    "    'neighbourhood': 'loc_neighborhood',\n",
    "    'suburb': 'loc_suburb',\n",
    "    'county': 'loc_county',\n",
    "    'city': 'loc_city',\n",
    "    'state': 'loc_state',\n",
    "    'ISO3166-2-lvl4': 'loc_iso3166_2_lvl4',\n",
    "    'postcode': 'loc_postcode',\n",
    "    'country': 'loc_country',\n",
    "    'country_code': 'loc_country_code',\n",
    "}\n",
    "\n",
    "mapping_fc_filming_locations = {\n",
    "    'location_id': 'fl_location_id',\n",
    "    'imdb_id': 'fl_imdb_id',\n",
    "}\n",
    "\n",
    "mapping_fc_restaurants = {\n",
    "    'restaurant_id': 'res_id',\n",
    "    'restaurant_name': 'res_name',\n",
    "    'legal_business_name': 'res_legal_business_name',\n",
    "    'doing_business_as_dba': 'res_doing_business_as_dba',\n",
    "    'seating_interest_sidewalk': 'res_seating_interest_sidewalk',\n",
    "    'landmarkdistrict_terms': 'res_landmarkdistrict_terms',\n",
    "    'location_id': 'res_location_id',\n",
    "}\n",
    "\n",
    "mapping_fc_actors = {\n",
    "    'actor_id': 'act_id',\n",
    "    'actor_name': 'act_name',\n",
    "}\n",
    "\n",
    "mapping_fc_genre = {\n",
    "    'genre_id': 'gen_id',\n",
    "    'genre': 'gen_name',\n",
    "}\n",
    "\n",
    "mapping_fc_genres_movies = {\n",
    "    'genre_id': 'gm_genre_id',\n",
    "    'imdb_id': 'gm_imdb_id',\n",
    "}\n",
    "\n",
    "mapping_fc_actors_movies = {\n",
    "    'actor_id': 'am_actor_id',\n",
    "    'imdb_id': 'am_imdb_id',\n",
    "}\n",
    "\n",
    "mapping_fc_movies = {\n",
    "    'imdb_id': 'mov_imdb_id',\n",
    "    'title': 'mov_title',\n",
    "    'year': 'mov_year',\n",
    "    'director': 'mov_director',\n",
    "    'writer': 'mov_writer',\n",
    "    'overview': 'mov_overview',\n",
    "    'rating': 'mov_rating',\n",
    "    'nb_users_ratings': 'mov_nb_users_ratings',\n",
    "}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eeb5efd20ee6ad6d",
   "metadata": {},
   "source": [
    "def rename_columns(df, column_mapping):\n",
    "    \"\"\"\n",
    "    Rename DataFrame columns using a mapping dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to rename.\n",
    "        column_mapping (dict): A dictionary mapping old column names to new ones.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with renamed columns.\n",
    "    \"\"\"\n",
    "    return df.rename(columns=column_mapping)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a474ce2a1fcc6c3",
   "metadata": {},
   "source": [
    "df_locations_renamed = rename_columns(df_standardized_locations, mapping_fc_locations)\n",
    "df_filming_locations_renamed = rename_columns(df_locations_movies, mapping_fc_filming_locations)\n",
    "df_restaurants_renamed = rename_columns(df_restaurants, mapping_fc_restaurants)\n",
    "df_actors_renamed = rename_columns(df_actors, mapping_fc_actors)\n",
    "df_genres_renamed = rename_columns(df_genres, mapping_fc_genre)\n",
    "df_movies_genres_renamed = rename_columns(df_movies_genres, mapping_fc_genres_movies)\n",
    "df_movies_actors_renamed = rename_columns(df_movies_actors, mapping_fc_actors_movies)\n",
    "df_movies_cleaned_renamed = rename_columns(df_movies_cleaned, mapping_fc_movies)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd965613f43f8e8",
   "metadata": {},
   "source": [
    "## Data Loading (db connection + INSERT)"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b838c50470bdabb",
   "metadata": {},
   "source": [
    "DATABASE_URL = f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@\" \\\n",
    "               f\"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n",
    "engine = create_engine(DATABASE_URL)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f82f65f55a462d5",
   "metadata": {},
   "source": [
    "def load_dataframe_to_postgres(df, table_name, engine, if_exists='append', primary_key_column=None):\n",
    "    \"\"\"\n",
    "    Load a pandas DataFrame into a PostgreSQL database, logging errors and continuing processing.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to load.\n",
    "        table_name (str): The target table name.\n",
    "        engine: The SQLAlchemy database engine.\n",
    "        if_exists (str): Behavior when the table exists. Options: 'fail', 'replace', 'append'.\n",
    "        primary_key_column (str): The name of the primary key column to log failing rows.\n",
    "    \"\"\"\n",
    "    if primary_key_column and primary_key_column not in df.columns:\n",
    "        raise ValueError(f\"Primary key column '{primary_key_column}' not found in the DataFrame.\")\n",
    "\n",
    "    successful_rows = []\n",
    "    failed_rows = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            row_df = pd.DataFrame([row])\n",
    "            row_df.to_sql(table_name, engine, index=False, if_exists=if_exists, method=\"multi\")\n",
    "            successful_rows.append(row[primary_key_column] if primary_key_column else index)\n",
    "        except IntegrityError as e:\n",
    "            engine.dispose()  # Dispose the engine to avoid locked connections\n",
    "            failed_rows.append({\n",
    "                \"row\": row.to_dict(),\n",
    "                \"primary_key\": row[primary_key_column] if primary_key_column else index,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "    print(f\"Successfully inserted rows: {len(successful_rows)}\")\n",
    "    print(f\"Failed rows: {len(failed_rows)}\")\n",
    "    #if failed_rows:\n",
    "    #    for failed_row in failed_rows:\n",
    "    #        print(f\"Failed primary key: {failed_row['primary_key']}, Error: {failed_row['error']}\")\n",
    "\n",
    "    return successful_rows, failed_rows\n",
    "\n",
    "def load_dataframe_to_postgres_batch(df, table_name, engine, if_exists='append', batch_size=1000):\n",
    "    \"\"\"\n",
    "    Load a pandas DataFrame into a PostgreSQL database, logging and returning failed batches.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to load.\n",
    "        table_name (str): The target table name.\n",
    "        engine: The SQLAlchemy database engine.\n",
    "        if_exists (str): Behavior when the table exists. Options: 'fail', 'replace', 'append'.\n",
    "        batch_size (int): The number of rows to insert in each batch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (successful_rows, failed_batches)\n",
    "    \"\"\"\n",
    "    successful_rows = 0\n",
    "    failed_batches = []\n",
    "\n",
    "    # Divide the DataFrame into batches\n",
    "    batches = [df[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "    for i, batch in enumerate(batches):\n",
    "        try:\n",
    "            # Batch insert using to_sql\n",
    "            batch.to_sql(\n",
    "                table_name,\n",
    "                engine,\n",
    "                index=False,\n",
    "                if_exists=if_exists,\n",
    "                method=\"multi\"  # Use the \"multi\" method for batch inserts\n",
    "            )\n",
    "            successful_rows += len(batch)\n",
    "        except IntegrityError as e:\n",
    "            engine.dispose()  # Dispose of the engine to avoid locked connections\n",
    "            #print(f\"Error during batch {i}: {e}\")\n",
    "            failed_batches.append({\"batch_index\": i, \"rows\": batch, \"error\": str(e)})\n",
    "\n",
    "    return successful_rows, failed_batches\n",
    "\n",
    "def reprocess_failed_batches(failed_batches, table_name, engine):\n",
    "    \"\"\"\n",
    "    Reprocess rows from failed batches by inserting them individually into the database.\n",
    "\n",
    "    Parameters:\n",
    "        failed_batches (list): A list of dictionaries containing failed batch details.\n",
    "        table_name (str): The target table name.\n",
    "        engine: The SQLAlchemy database engine.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (successful_rows, retry_failed_rows)\n",
    "    \"\"\"\n",
    "    successful_rows = 0\n",
    "    retry_failed_rows = []\n",
    "\n",
    "    for failed_batch in failed_batches:\n",
    "        batch_index = failed_batch[\"batch_index\"]\n",
    "        batch_rows = failed_batch[\"rows\"]\n",
    "\n",
    "        print(f\"Retrying rows from failed batch {batch_index} individually...\")\n",
    "        for _, row in batch_rows.iterrows():\n",
    "            try:\n",
    "                # Insert each row individually\n",
    "                row_df = pd.DataFrame([row])\n",
    "                row_df.to_sql(\n",
    "                    table_name,\n",
    "                    engine,\n",
    "                    index=False,\n",
    "                    if_exists='append',\n",
    "                    method=\"multi\"\n",
    "                )\n",
    "                successful_rows += 1\n",
    "            except IntegrityError as e:\n",
    "                retry_failed_rows.append({\n",
    "                    \"row\": row.to_dict(),\n",
    "                    \"batch_index\": batch_index,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "                print(f\"Error inserting row in batch {batch_index}: {e}\")\n",
    "\n",
    "    return successful_rows, retry_failed_rows"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e0252005ec06bcb",
   "metadata": {},
   "source": [
    "dataframes = {\n",
    "    \"fc_locations\": df_locations_renamed,\n",
    "    \"fc_filming_locations\": df_filming_locations_renamed,\n",
    "    \"fc_restaurants\": df_restaurants_renamed,\n",
    "    \"fc_actors\": df_actors_renamed,\n",
    "    \"fc_genres\": df_genres_renamed,\n",
    "    \"fc_genres_movies\": df_movies_genres_renamed,\n",
    "    \"fc_actors_movies\": df_movies_actors_renamed,\n",
    "    \"fc_movies\": df_movies_cleaned_renamed,\n",
    "}\n",
    "for name, dataframe in dataframes.items():\n",
    "    print(f\"Number of rows in {name}: {len(dataframe)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "967456d312ee3ef0",
   "metadata": {},
   "source": [
    "# Load the DataFrames into the database\n",
    "\n",
    "print(\"\\nLoading movies...\")\n",
    "successful_movies, failed_movies = load_dataframe_to_postgres_batch(df_movies_cleaned_renamed,'fc_movies',engine,batch_size=500)\n",
    "\n",
    "print(\"Loading locations...\")\n",
    "successful_locations, failed_locations = load_dataframe_to_postgres_batch(df_locations_renamed,'fc_locations',engine,batch_size=50)\n",
    "\n",
    "print(\"\\nLoading restaurants...\")\n",
    "successful_restaurants, failed_restaurants = load_dataframe_to_postgres_batch(df_restaurants_renamed,'fc_restaurants',engine,batch_size=20)\n",
    "\n",
    "print(\"\\nLoading Genres...\")\n",
    "successful_genres, failed_genres = load_dataframe_to_postgres_batch(df_genres_renamed,'fc_genres',engine,batch_size=5)\n",
    "\n",
    "print(\"\\nLoading Actors...\")\n",
    "successful_actors, failed_actors = load_dataframe_to_postgres_batch(df_actors_renamed,'fc_actors',engine,batch_size=5000)\n",
    "\n",
    "print(\"\\nLoading Movies-Genres...\")\n",
    "successful_genres_movies, failed_genres_movies = load_dataframe_to_postgres_batch(df_movies_genres_renamed, 'fc_genres_movies', engine, batch_size=800)\n",
    "\n",
    "print(\"\\nLoading Movies-Actors...\")\n",
    "successful_movies_actors, failed_movies_actors = load_dataframe_to_postgres_batch(df_movies_actors_renamed, 'fc_actors_movies', engine, batch_size=8000)\n",
    "\n",
    "print(\"\\nLoading Filming Locations...\")\n",
    "successful_filming_locations, failed_filming_locations = load_dataframe_to_postgres_batch(df_filming_locations_renamed, 'fc_filming_locations', engine, batch_size=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4fd92ba2e5276308",
   "metadata": {},
   "source": [
    "# Summary Logs\n",
    "print(f\"Locations: {(successful_locations)} inserted, {len(failed_locations)} failed\")\n",
    "print(f\"Restaurants: {(successful_restaurants)} inserted, {len(failed_restaurants)} failed\")\n",
    "print(f\"Movies: {(successful_movies)} inserted, {len(failed_movies)} failed\")\n",
    "print(f\"Genres: {(successful_genres)} inserted, {len(failed_genres)} failed\")\n",
    "print(f\"Actors: {(successful_actors)} inserted, {len(failed_actors)} failed\")\n",
    "print(f\"Movies-Genres: {(successful_genres_movies)} inserted, {len(failed_genres_movies)} failed\")\n",
    "print(f\"Movies-Actors: {(successful_movies_actors)} inserted, {len(failed_movies_actors)} failed\")\n",
    "print(f\"Filming Locations: {(successful_filming_locations)} inserted, {len(failed_filming_locations)} failed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2cc96f22feca55a2",
   "metadata": {},
   "source": [
    "Code to reprocess failed batches\n",
    "```python\n",
    "# Reprocess failed batches later\n",
    "if failed_locations:\n",
    "        reprocess_successful, retry_failed_rows = reprocess_failed_batches(failed_locations, \"fc_locations\", engine)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "51558a9bb6938c0",
   "metadata": {},
   "source": [
    "def update_geography(engine):\n",
    "    \"\"\"\n",
    "    Updates the loc_geography column in fc_locations table using ST_SetSRID and ST_MakePoint.\n",
    "    \"\"\"\n",
    "\n",
    "    update_query = \"\"\"\n",
    "    UPDATE fc_locations\n",
    "    SET\n",
    "        loc_geography = ST_SetSRID(ST_MakePoint(loc_longitude, loc_latitude), 4326)::geography\n",
    "    WHERE\n",
    "        loc_latitude IS NOT NULL\n",
    "      AND loc_longitude IS NOT NULL;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as connection:\n",
    "            connection.execute(text(update_query))\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"An error occurred while updating loc_geography: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "update_geography(engine)",
   "id": "4aeca7269dd70e34",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
